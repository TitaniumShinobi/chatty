LoRA (Low-Rank Adaptation) training is an efficient method to fine-tune large AI models (like Stable Diffusion) by injecting small, trainable low-rank matrices, rather than adjusting all model weights, allowing customization for specific subjects or styles with minimal parameters, faster training, and much smaller output files. The process involves preparing an image dataset with captions, using software (like Kohya's GUI or Civitai's Trainer), configuring settings (learning rate, rank), and training to create a lightweight, shareable LoRA file that modifies the base model's output.  [1]  
This video provides a step-by-step tutorial on training a LoRA: https://www.youtube.com/watch?v=m3ENCAwWDXc (https://www.youtube.com/watch?v=m3ENCAwWDXc) 
Core Concept 

• Low-Rank Matrices: Instead of full fine-tuning, LoRA adds small, trainable matrices (A and B) to specific layers (like attention) of the frozen base model. 
• Parameter Efficiency: Only these small matrices are trained, drastically reducing the number of trainable parameters (often by 90%+) and the GPU memory needed. 

The Training Process (Simplified) 

1. Dataset Preparation: 
	• Gather high-quality images (20-100+) of your subject/style. 
	• Caption each image (using tools like BLIP) describing everything except the subject, using a unique trigger word for the subject (e.g., sks_person). 

2. Software Setup: Use tools like Kohya's GUI, ComfyUI, or online trainers (Civitai, SeaArt). 
3. Configuration: 
	• Base Model: Select your base model (SD 1.5, SDXL). 
	• Rank (Dimension): Determines complexity (e.g., 8, 16, 32, 64). 
	• Learning Rate (LR): Crucial for preventing overfitting (e.g., 1e-4 or lower). 
	• Steps/Epochs: How many times the model sees the data (adjust for convergence). 
	• Regularization Images (Optional): General images (e.g., "man," "woman") to prevent the model from forgetting the base concept. 

4. Training: Run the trainer; it iteratively adjusts the LoRA weights to minimize the difference (loss) between generated images and your dataset, guided by captions. 
5. Testing: Generate images with your trigger word and experiment with LoRA weights (0.6-1.0) to find the best balance. 

Why Use LoRA? 

• Small Files: LoRAs are typically a few hundred MB, easy to share. 
• Fast & Efficient: Faster training and lower hardware cost than full fine-tuning. 
• Versatile: Easily combined with other techniques or applied to different base models. 

AI responses may include mistakes.

[1] https://docs.conductortech.com/ai/tutorials/lora/

Not all images can be exported from Search.
