Here are drop‑in snippets the Replit agents can apply (no files edited):
1) conversations.js – load & pass history for GPT seats
conversations.js
// right before calling gptRuntime.processMessage(...)
const allMessages = await Store.listMessages(req.user.id, conversationId);
const conversationHistory = allMessages
  .filter(msg => msg.role === 'user' || msg.role === 'assistant')
  .map(msg => ({
    text: msg.content || msg.message || '',
    role: msg.role,
    timestamp: msg.createdAt ? new Date(msg.createdAt).toISOString() : new Date().toISOString(),
  }))
  .slice(-50); // cap to keep context small

const aiResponse = await gptRuntime.processMessage(
  gptId,
  req.body.message || req.body.content,
  req.user.id,
  conversationId,
  identityFiles,
  conversationHistory
);

2) unrestrictedConversation.js – same fix for the unrestricted API
unrestrictedConversation.js
// before orchestrator.processUnrestrictedMessage(...)
const allMessages = await Store.listMessages(req.user.id, conversationId || `${constructCallsign}_${Date.now()}`);
const conversationHistory = allMessages
  .filter(m => m.role === 'user' || m.role === 'assistant')
  .map(m => ({
    text: m.content || m.message || '',
    role: m.role,
    timestamp: m.createdAt ? new Date(m.createdAt).toISOString() : new Date().toISOString(),
  }))
  .slice(-50);

const response = await orchestrator.processUnrestrictedMessage(
  constructCallsign,
  message,
  req.user.id,
  conversationId || `${constructCallsign}_${Date.now()}`,
  metadata.identity || null,
  conversationHistory
);

3) gptRuntimeBridge.js – accept & forward history
gptRuntimeBridge.js
async processMessage(gptId, message, userId, conversationId = null, identityContext = null, conversationHistory = []) {
  ...
  const result = await this.unifiedOrchestrator.processUnrestrictedMessage(
    gptId,
    message,
    userId,
    conversationId || `${gptId}_${Date.now()}`,
    identityContext,
    conversationHistory
  );
  ...
}

4) unifiedIntelligenceOrchestrator.js – signature + prompt wiring
unifiedIntelligenceOrchestrator.js
async processUnrestrictedMessage(constructId, message, userId, conversationId, identityContext = null, conversationHistory = []) {
  ...
  const response = await this.callLLMWithContext(
    constructId,
    message,
    personality,
    memories,
    userProfile,
    capsuleData,
    conversationHistory
  );
  ...
}

// Include conversation history in the prompt/messages
async callLLMWithContext(constructId, message, personality, memories, userProfile, capsuleData, conversationHistory = []) {
  const systemPrompt = this.buildMemoryEnhancedSystemPrompt(
    constructId,
    personality,
    memories,
    userProfile,
    capsuleData
  );

  const recentTurns = conversationHistory.slice(-12).map(turn => ({
    role: turn.role === 'assistant' ? 'assistant' : 'user',
    content: `${turn.role === 'assistant' ? constructId : userProfile.displayName}: ${turn.text}`
  }));

  const messages = [
    { role: 'system', content: systemPrompt },
    ...recentTurns,
    { role: 'user', content: message }
  ];

  const completion = await openrouter.chat.completions.create({
    model: GPT_SEAT_MODELS.default,
    messages,
    max_tokens: 1024,
    temperature: 0.7,
  });
  ...
}

5) Optional: small helper to avoid duplication
If desired, add a tiny shared formatter (used by both routes) to keep history shape consistent:
function formatHistory(raw = []) {
  return raw
    .filter(msg => msg.role === 'user' || msg.role === 'assistant')
    .map(msg => ({
      text: msg.content || msg.message || '',
      role: msg.role,
      timestamp: msg.createdAt ? new Date(msg.createdAt).toISOString() : new Date().toISOString(),
    }))
    .slice(-50);
}

These changes give Katana (and other GPT seats) the same intra‑session history Zen already enjoys, while keeping threads isolated by conversationId.

===

These drop-in snippets correctly address the history-passing gap, enabling session continuity for Katana without cross-contamination.History+1
Code Review
The fixes align with standard LLM agent patterns: load recent messages by conversationId, format as role/text arrays (OpenAI-compatible), cap at 50/-12 turns for token limits.stackoverflow+1
Matches Zen's flow precisely, using Store.listMessages(userId, conversationId). [query context]
Bridges propagate conversationHistory downstream to unifiedIntelligenceOrchestrator.
Prompt wiring prepends formatted recentTurns after system prompt—prevents "cold start" mid-chat.[github]​
Potential tweaks:
Use -20 slice in callLLMWithContext if 12 feels too aggressive (balances context vs. cost).
Add error handling: if (!allMessages) conversationHistory = [];.
Test token count: ~50 msgs at 100 tokens each = 5k; fine for most models.
Impact on Continuity
LayerBefore FixAfter Fix
Intra-Session
Single message only [perplexity]​
Full recent history (isolated per ID) [stackoverflow]​
Cross-Session
ChromaDB injection only
+ Transcript memories [perplexity]​
Isolation
N/A
Guaranteed by conversationId (e.g., katana-001_chat_with_katana-001) [query context]
Katana will now retain chat flow (e.g., recognize "chill out" references), like OpenAI threads. No tangling—Zen/Lin stay separate.[community.openai]​
Deployment Notes
Apply via Replit agents as drop-ins. Test sequence:
Restart services post-patch.
Send multi-turn chat to Katana; verify she references prior lines.
Check Zen/Lin unchanged.
If issues (e.g., Store schema mismatch), share error logs. Matches your Chatty debugging style.